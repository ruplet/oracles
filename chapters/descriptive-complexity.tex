\chapter{Descriptive Complexity}\label{chap:descriptive-complexity}
In the rest of this book, we usually measure complexity of algorithms:
how much time and memory will a given algorithm need to run to solve a problem of size \(n\)?
In this chapter we will instead focus on the complexity of defining the problem itself.

The central problem of Descriptive Complexity is to characterize a
complexity class by the \emph{power of logic required to define its problems}.
This is in contrast to \emph{Implicit Complexity Theory}, discussed later in~\autoref{sec:icc-rec}
and~\autoref{sec:icc-linear},
which seeks the weakest system sufficient to \emph{implement algorithms}
of the class; and in contrast to \emph{Bounded Arithmetic}, which studies
the weakest \emph{theory} required to define a function \emph{and prove its correctness}.
This perspective has led to elegant logical characterizations of many traditional
complexity classes, as we will discuss in~\autoref{sec:descriptive-results}.

\begin{remark}
In Descriptive Complexity, there is no notion of proofs of the sentences that we will study.
The problem of deciding if a given sentence is a tautology assuming
a particular proof system, is completely independent of this chapter's considerations,
and will be studied by us only in~\autoref{chap:bounded-arithmetic}.
\end{remark}


\subsection{Structures considered}
% IMPORTANT: what is bit? what is <=, +, -.
% section 1.2 of immerman
% bit(i, 0) holds iff i is odd.
% PLUS(i, j, k) meaning i + j = k
% 2. TIMES(i, j, k) meaning i x j = k
% 3. BIT(i, j) meaning bit j in the binary representation of i is

% this PLUS, TIMES seems to be like operations on unary numbers. 
% This is unary numbers because we can only do it up until `n` 
% (because we only have forall x, PLUS(x), PLUS(0), PLUS(1), PLUS(max),...)
In this chapter, we only study structures that satisfy the following criteria:
\begin{enumerate}
    \item \todo[inline]{finish this}
    \item ordered
    \item at least 2 elements
\end{enumerate}
For discussion on reasonability of these assumptions,
see~\cite[Proviso~1.14]{Immerman1999-IMMDC} and~\cite[Proviso~1.15]{Immerman1999-IMMDC}.



\begin{remark}[Ordered and Unordered Structures]\label{remark:unordered-structures}
Question 12.1. Is there a recursively enumerable listing of a set of sentences
from FO(LFP) that describes exactly all the polynomial-time, order-independent
boolean queries?
Fagin's theorem (Theorem 7.8) does not require an ordering because second-order ex-
istentiallogic is powerful enough to existentially quantify a linear ordering on the universe,
which the original proof of Fagin's Theorem does.

\paragraph{The Quest for a Logic Capturing \complexity{PTIME}}
Without an explicit ordering, \(\mathrm{FO[LFP]}\) does \textbf{not} capture
\(\complexity{PTIME}\)~\cite{Cai1992}.
The existence of a logic that characterizes \(\complexity{PTIME}\) on
\emph{unordered structures} remains a major open problem in computer science as of 2025.
An interesting complexity class is \complexity{inv-P} of permutation-invariant polynomial-time problems.
A good overview of this problem is~\cite[Chapter 12, The Role of Ordering]{Immerman1999-IMMDC}.
\end{remark}
% % - permutation-invariant PTIME
% %     The class `inv-P` of graph permutation-invariant problems decidable in polynomial time, is conjectured
% %     to not be characterizable this way. This is a restatement of the problem of finding a "logic capturing PTIME on unordered structures".
% %     A good discussion of this problem is present in Anuj Dawar's presentation from 2012: [@dawar2012syntactic].


% In a paper from 2019, Jean-Yves Moyen and Jakob Grue Simonsen discuss the problem of providing syntax for PTIME ,
% concluding that in the light of their generalization of Rice's theorem \cite{10.1007/978-3-030-22996-2_19}.
% (problem w reprezentacji:
% mozemy miec jezyk programowania dla P, ale odkladamy nierozstrzygalnos do problemu sprawdzenia czy mozna
% przetlumaczyc dana maszyne turinga na ten jezyk)





\section{First-order expressible decisional problems}
The definition below is from~\cite[Definition~4.24]{Immerman1999-IMMDC}.

\begin{definition}[\texorpdfstring{\(\complexity{FO}[t(n)]\)-complexity}{FO[t (n)]-complexity}]
Let $\mathcal{L}$ be a vocabulary and let $S$ be an $\mathcal{L}$-structure.  
We say that $S$ is a member of $\mathrm{FO}[t(n)]$ iff there exist

\todo{will this be roman with my driver.tex settings?}
\todo{consistency: bar c vs c1, c2,...,ck.}
\begin{enumerate}
  \item quantifier-free $\mathcal{L}$-formulas $M_i$ ($0 \le i \le k$);
  \item a tuple $\bar c$ of constants, and
  \item a quantifier block
  \[
     QB \;=\; (Q_1 x_1 . M_1)\,\dots\,(Q_k x_k . M_k)
  \]
\end{enumerate}

such that for all $\mathcal{L}$-structures $\mathcal{A}$,
\[
  \mathcal{A} \in S
  \quad\Longleftrightarrow\quad
  \mathcal{A} \models \bigl([QB]^{\,t(|A|)} M_0\bigr)(\bar c/\bar x).
\]

The substitution of constants $\bar c/\bar x$ is needed because the quantifier block
$QB$ may contain free variables that must be instantiated in order to obtain
a sentence. Note that even though the expression grows with \(k\),
the overall number of variables of the formula is constant, as it is a constant
quantifier block \(QB\) being iterated.

For our purposes, the most important case is \(t(n) = \bigO(1)\), then \(FO[t(n)] = FO[1]\)
for any input structure size uses exactly the same formula.
\end{definition}








\section{Defining functional problems in logic}

\subsection{\complexity{FO}-reductions}
Historically, introduced as the class of first-order queries
in~\cite[Definition~1.26]{IMMERMAN198686}.

\todo[inline]{semantic notion of definability, i.e. a function is definable means}
its graph is expressible by a formula in some logic.

This characterization will later be used (Definition~V.2.3 in~\cite{Cook_Nguyen_2010})
to define functions in \(\complexityi{FAC}{0}\) as those of polynomially bounded output length
whose \emph{bit-graphs} lie in \(\complexityi{AC}{0}\).
That is how we will transition to studying functional complexity classes.

\subsection{First-Order Projections (fops)}

First-order projections (fops) and quantifier-free projections (qfps) are
defined in~\cite[Definition~11.7]{Immerman1999-IMMDC}.

The following interesting property of first-order projections, which says that
there is only one complete problem via first-order projections for each ``nice''
complexity class.
Let \(\texttt{C}\) be one of the complexity classes \complexityi{NC}{1}, \complexity{L}, \complexity{NL},
\complexity{P}, \complexity{NP}, \complexity{PSPACE}.
Let \(\texttt{A}\) and \(\texttt{B}\) be problems complete for \complexity{C} via fops. Then there is a first-order
definable isomorphism between \(\texttt{A}\) and \(\texttt{B}\)~\cite[Fact~11.16]{Immerman1999-IMMDC}.

Most of natural NP-complete problems are already complete under fops.
SAT is NP-complete via fops and, in fact, via qfps~\cite[Proposition~11.10]{Immerman1999-IMMDC}.

There is a problem \(\texttt{S}\) that is \complexity{NP}-complete via first-order
reductions, but not via fops\cite[Proposition~11.14]{Immerman1999-IMMDC}.


% definition of fo-projection, of uniformity and of ac0:definition 6,  https://drops.dagstuhl.de/storage/00lipics/lipics-vol062-csl2016/LIPIcs.CSL.2016.20/LIPIcs.CSL.2016.20.pdf

\subsection{FO transductions}
Defined e.g.\ in~\cite[Section~2]{nesetril2021structuralpropertiesfirstordertransduction}. They are,
however, completely different anything we study in this work and are defined as compositions of
\emph{copying}, \emph{coloring} and \emph{simple interpretations}, which we will not discuss.

\subsection{MSO transductions}
Defined e.g.\ in~\cite[Section~2]{COURCELLE199453}.\todo[inline]{Write one paragraph about what it is, don't define}
% https://www.mimuw.edu.pl/~bojan/posts/who-to-cite-mso-transductions



% TODO: Describe FO-uniformity via first-order queries $I : \mathrm{STRUC}[ts] \rightarrow \mathrm{STRUC}[tc]$ with $I(0^n) = C_n$ (Definition~5.16 in~\cite{Immerman1999-IMMDC}).
% TODO: Restate that FO-reductions are defined as first-order queries (Definition~1.26 in~\cite{Immerman1999-IMMDC}).













\section{Results}\label{sec:descriptive-results}

\subsection{FO = AC0}\label{sec:fo-eq-ac0}
This is stated by~\cite[Corollary~5.32]{Immerman1999-IMMDC}, which is
implied from~\cite[Theorem~5.22]{Immerman1999-IMMDC} and earlier from~\cite[Theorem~5.2]{Immerman1999-IMMDC}.
For comparison of their definition of \complexity{FO}-uniform \complexityi{AC}{0} and ours, refer to~\cite[Definition~5.17]{Immerman1999-IMMDC}
and~\autoref{def:aci}.
% FO is in AC:~\cite[Lemma~5.4]{Immerman1999-IMMDC}.
% AC is in FO:~\cite[Lemma~5.3]{Immerman1999-IMMDC} for ind in fo, then~\cite[Lemma~4.25]{Immerman1999-IMMDC}.

As an example, Grädel's Theorem in descriptive complexity states that
\(\complexity{NL}\) is the class of finite models of the second-order Krom formulas
\cite{GRADEL199235}.
Here, a \emph{Krom formula} is a formula in conjunctive normal form (CNF)
where each clause contains at most
two literals. The satisfiability problem for Krom formulas, Krom-SAT, is
complete for \(\complexity{coNL}\) (or equivalently \(\complexity{NL}\), by the Immerman-Szelepcsényi
Theorem).

% second-order horn formula: https://en.wikipedia.org/wiki/Second-order_propositional_logic
Similarly, Grädel~\cite{GRADEL199235} shows that second-order Horn formulas
express precisely the properties decidable in polynomial time.
A \emph{Horn clause} is a clause with at most one positive literal and any number
of negative literals, and a Horn formula is a conjunction of such clauses.
By a result independently proved by Immerman~\cite{IMMERMAN198686}
and Vardi~\cite{10.1145/800070.802186},
\(\complexity{PTIME}\) is also characterized by first-order logic with a least fixed-point operator
(\(\mathrm{FO[LFP]}\)).
Similar results exist for most commonly studied complexity classes:
\(\complexity{NP}\) (Fagin's theorem), \(\complexity{coNP}\), \(\complexity{PH}\), \(\complexity{PSPACE}\), and \(\complexity{EXPTIME}\).

For an accessible introduction, see the classical and self-contained reference
by Immerman~\cite{Immerman1999-IMMDC}.

Fagin's theorem says that existential second-order logic characterizes
\(\complexity{NP}\).
It is worth noting that this theorem does \emph{not} assume order on the input ---
intuitively, \(\complexity{NP}\) is strong enough to ``guess'' the order relation.


A concise overview of the classical results is in~\cite[Section~15.1]{Immerman1999-IMMDC},
where characterizations of \(\complexity{DSPACE}(n^k)\), \complexity{L},
\complexity{NL}, \complexity{P}, \complexity{NP} and \complexity{PSPACE} are described.
A characterization of a complexity class \complexity{FO} is also discussed there, which we will introduce
in this chaper.












\section{Descriptive Complexity for programming languages}
Syntax is just syntax for logic, and the interpreter
is a proof of the theorem FOTC=NL. We compile to operational semantics.
We have Datalog programming language. It tells us that if the only programs
we wish to write are in NL, we can specify all of them in FOTC and it will be just enough!


\subsection{Importance of \texorpdfstring{\complexity{MSO} = \complexity{REG}}{MSO = REG}}\label{subsec:mso-eq-reg}
\todo[inline]{While the equality is extensionally true, the compilation of mso to dfa is superexponential.}


\subsection{Descriptive Complexity and Type Systems}
Could descriptive complexity be used as specification mechanism of a language?
After investigating systems within descriptive complexity, we explored
whether their logical frameworks could be combined with type-theoretic
approaches to design programming languages that capture complexity classes.
While this attempt to bridge descriptive complexity and type systems
was conceptually motivated, the two fields remain quite distant in practice,
and we were not successful in fully integrating them.
Nonetheless, this line of inquiry led to a broader investigation of
type systems that enforce resource bounds --- particularly those inspired
by linear logic and implicit computational complexity.
The results of this exploration are presented in the next chapter.
% - czy potrzebujemy systemu typów w języku programowania? Nie\! To najważniejsze niepoprawne założenie które popełniamy. Potrzebujemy systemu specyfikacji, ale system typów wcale nie musi być najłatwiejszą ani najbardziej wszechstronną metodą umożliwienia użytkownikowi automatycznego rozumowania o cechach programu. Leslie Lamport porusza tę kwestię w swoim artykule ([https://lamport.azurewebsites.net/pubs/lamport-types.pdf](https://lamport.azurewebsites.net/pubs/lamport-types.pdf) ). Ponadto, systemy typów są bardzo trudne do poprawnego zaprojektowania, zrozumienie nawet nieszczególnie silnych systemów wymaga lat doświadczenia  

% Specification langs:  
% can use set theory for this\!  
% [https://arxiv.org/pdf/cs/9511102](https://arxiv.org/pdf/cs/9511102)  


\subsection{Datalog}
% TODO: FO(LFP) = Datalog;
% For example, Datalog is known to capture precisely the same predicates about the input as PTIME
% \cite[Theorem 4.4]{10.1145/502807.502810}