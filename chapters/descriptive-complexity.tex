\chapter{Descriptive Complexity}\label{chap:descriptive-complexity}
In the rest of this thesis, we usually measure complexity of algorithms:
how much time and memory will a given algorithm need to run to solve a problem of size \(n\)?
In this chapter we will instead focus on the complexity of defining the problem itself.

The central problem of Descriptive Complexity is to characterize a
complexity class by the \emph{power of logic required to define its problems}.
This is in contrast to \emph{Implicit Complexity Theory}, discussed later in~\autoref{chap:icc},
which seeks the weakest system sufficient to \emph{implement algorithms}
of the class; and in contrast to \emph{Bounded Arithmetic}, which studies
the weakest \emph{theory} required to define a function \emph{and prove its correctness}.
This perspective has led to elegant logical characterizations of many traditional
complexity classes, as we will discuss in~\autoref{sec:descriptive-results}.

Before going into details, we will discuss an example to recall what it means for
a structure to model a formula~(\autoref{def:L-structure}):

\begin{remark}
In Descriptive Complexity, there is no notion of a proof.
The problem of deciding if a given sentence is provable is independent of this chapter's considerations,
and will be studied by us only in~\autoref{chap:bounded-arithmetic}.
\end{remark}

\begin{example}\label{exm:logic-simple-model}
Consider a vocabulary \(\mathcal{L}\) consisting of unary relations \(\mathrm{Zero}(x), \mathrm{One}(x)\) and binary relations
\(\mathrm{=}, \mathrm{\leqslant}\). If we think of positions in a binary string as elements of the universe,
we can define e.g. for a string \(01011\) an \(\mathcal{L}\)-structure \(\mathcal{M}\) with:
\begin{enumerate}
\item universe \(\{1, 2, 3, 4, 5\}\);
\item \(\mathrm{Zero} := \{1, 3\}; \mathrm{One} := \{2, 4, 5\}; \mathrm{=} := \{\langle1, 1\rangle, \langle2, 2 \rangle, \dots, \langle5, 5 \rangle\}; \mathrm{\leqslant} := \{\dots\}\).
\end{enumerate}

Now, we can reason about the original binary string using logic.
The formula  \(\exists x \ldotp \mathrm{One}(x)\) is true in \(\mathcal{M}\). However, the formula
\(\forall x \ldotp \mathrm{Zero}(x)\) is not.
It is insightful to analyze in general what kind of logical formulas are true in \(\mathcal{M}\).
\end{example}


We can use the language of logic to describe computational queries about the underlying structure.
This is the intuition behind the languages designed for querying databases. To formally connect
logic and computation, consider the following

\begin{definition}[Language of binary strings]
  Consider a vocabulary \(\tau_{\text{string}}\) to contain only the unary relations \(\mathrm{Zero}(x), \mathrm{One}(x)\) and
  the binary relations \(=, \leqslant\). Intuitively, \(x \leqslant y\) means that memory cell $x$ comes before
  memory cell $y$. As we can represent most of the real-world structures of interest as some binary
  string on a computer, this simple vocabulary already allows us to ask interesting questions.
\end{definition}

\begin{definition}[Language generated by a formula]\label{def:language-decided-by-formula}
  For a binary string \(w \in \{0, 1\}^n\) define its corresponding structure \(\mathcal{M}_w\)
  over vocabulary \(\tau_{\text{string}}\)
  to have the universe \(M := \{1, \dots, n\}\) and the standard semantics of relations from \(\tau_{\text{string}}\).
  
  We define the language \(L_\varphi\) to be \emph{generated} by a first-order sentence
  $\varphi$ over $\tau_{\text{string}}$
  iff
  \[\mathcal{M}_w \models \varphi \;\Longleftrightarrow\; w \in L_\varphi.\]
\end{definition}

Now let's shift our focus to a more standard definition.

\begin{definition}[First order logic on ordered structures]\label{def:fo-order}
  Define the vocabulary $\tau_\leqslant$ to contain:
  \begin{enumerate}
  \item the constants $0, 1, \mathrm{max}$;
  \item the binary relations $=, \leqslant$.
  \end{enumerate}

  \complexity{FO_\leqslant} is the class of sentences of first-order logic in the language \(\tau_\leqslant\).

  From now on, whenever talking about the semantics of any such sentence, we will require
  the elements of the (finite) universe to be interpreted as actual natural numbers $0, 1, \dots$;
  the equality and order to be interpreted as the actual equality and order on the elements of the model;
  $0, 1, \mathrm{max}$ to be interpreted as the minimum, second, and maximum elements under $\leqslant$.

  \begin{remark}[Bibliography]
    In literature, \(\complexity{FO_\leqslant}\) is called \complexity{FO (wo~BIT)}; see~\cite[Ordering~Proviso~1.14]{Immerman1999-IMMDC}.
  \end{remark}
\end{definition}


\begin{definition}[First order logic with arithmetical predicates]\label{def:fo-arith}
  \logicFO{} is the class of sentences of first-order logic over $\tau_\leqslant$ extended with
  the binary relation $\mathrm{BIT}$.
  Semantically, we require \(\mathrm{BIT}(x, y)\) to hold iff bit $y$ in the binary representation of $x$ is 1.

  \begin{remark}
    By default in the literature, the arithmetic predicates and order are included.
  Usually, arithmetic predicates $\mathrm{PLUS}(x, y, z)$, $\mathrm{TIMES}(x, y, z)$, $\mathrm{SUCC}(x, y)$, denoting
  that $x + y = z, x * y = z, x + 1 = y$ respectively, are also added. We use the fact that $\mathrm{PLUS}, \mathrm{TIMES}$ are
  first-order definable from $\mathrm{BIT}$~\cite[Theorem~1.17]{Immerman1999-IMMDC} and that $\mathrm{SUCC}$ is
  first-order definable from $\leqslant$~\cite[Section~1.2]{Immerman1999-IMMDC}.
  \end{remark}
  % \cite[Proposition~9.16]{Immerman1999-IMMDC}: BIT is definable in FO(wo BIT)(DTC), so also (TC) and (LFP).
  % SOLUTION: JUST DON'T CARE ABOUT THIS! JUST ADD BIT NORMALLY TO LOGIC WITH ZERO, ONE.
\end{definition}



% \section{First-order expressible decisional problems}
% The definition below is from~.

% \begin{definition}[{%
%   \cite[Definition~4.24]{Immerman1999-IMMDC}}%
%   ~The complexity class \complexity{FO[t (n)]}%
% ]
% Let $\mathcal{L}$ be a vocabulary and let $S$ be a class of $\mathcal{L}$-structures. 
% We say that $S$ is of complexity $\complexity{FO[t(n)]}$ iff there exist

% \todo{consistency: bar c vs c1, c2,\dots,ck.}
% \begin{enumerate}
%   \item quantifier-free $\mathcal{L}$-sentences $M_i$ ($0 \le i \le k$);
%   \item a quantifier block
%   \[
%      QB \;=\; (Q_1 x_1 \ldotp M_1)\,\dots\,(Q_k x_k \ldotp M_k)
%   \]
% \end{enumerate}

% such that for all $\mathcal{L}$-structures $\mathcal{A}$,
% \[
%   \mathcal{A} \in S
%   \quad\Longleftrightarrow\quad
%   \mathcal{A} \models [QB]^{\,t(|A|)} M_0.
% \]

% \begin{remark}
% In the original definition, free variables in $M_i$ are allowed which need to be treated. We ignore them here.
% \end{remark}

% In particular, for \(t(n) = \bigO(1)\), then \(\complexity{FO[ t(n)]} = \complexity{FO}\) are formulas
% with the same single block of quantifiers for all input sizes. This case will be
% the most important for~\autoref{thm:}.
% \end{definition}








\section{Results}\label{sec:descriptive-results}
The field of descriptive complexity has a long history and we should not introduce all the results
in this work, as our primary purpose is to examine if, in the first place, they are relevant to
our problem. We will only describe the logical characterizations of the classes that are the
most interesting for us: \compP{} and uniform \complexityi{AC}{0} which will underpin our~\autoref{chap:bounded-arithmetic}.
A~concise overview of the classical results is presented in~\cite[Section~15.1]{Immerman1999-IMMDC},
where characterizations of \(\complexity{SPACE}(n^k)\), \complexity{L},
\complexity{NL}, \complexity{P}, \complexity{NP} and \complexity{PSPACE} are described.
Please also see the insightful~\cite{doi:10.1137/0216051} that introduces
model-theoretical characterizations of numerous decisional complexity classes. 


One intensely studied logic is \logicFOLFP{}, defined inductively.
\begin{samepage}
\begin{definition}[{\cite[Definition~4.5]{Immerman1999-IMMDC}~Least fixed-point logic}]
\logicFOLFP{} is a class of logical sentences such that:
\begin{enumerate}
  \item if \(\varphi \in \logicFO{}\) then \(\varphi \in \logicFOLFP{}\);
  \item if \(\varphi_R(x_1, \dots, x_k) \in \logicFOLFP{}\), where \(R\) is a \(k\)-ary relation
    and \(R\) only occurs positively in $\varphi_R(x_1, \dots, x_k)$ (i.e.\ every occurrence of $R$ is preceded
    by an even number of negations), then \(\complexity{LFP}_{R(x_1, \dots, x_k)} \varphi\) may be used
    as a new $k$-ary relation symbol denoting the least fixed-point of $\varphi$.
\end{enumerate}
\end{definition}
\end{samepage}

\begin{theorem}[$\logicFOLFP{} = \compP$]\label{thm:fo-lfp-eq-ptime}
  The class of languages generated by
  sentences from \logicFOLFP{} is precisely \(\complexity{P}\).

  \begin{remark}
    We can think of the $\complexity{LFP}$ operator as allowing us to write recursive formulas.
  \end{remark}

  \begin{remark}
    This result has been proved independently by Immerman~\cite{IMMERMAN198686}
    and Vardi~\cite{10.1145/800070.802186}. For a more uniform treatment, see~\cite[Theorem~4.10]{Immerman1999-IMMDC}.
  \end{remark}
\end{theorem}

\begin{theorem}[{%
  \cite[Corollary~5.32]{Immerman1999-IMMDC}~\logicFO{}
  = \complexity{FO}-uniform \complexityi{AC}{0}%
}]\label{thm:fo-eq-ac0}
Languages \(L \subseteq \{0, 1\}^\ast\) decidable by uniform \complexityi{AC}{0}
  circuits are precisely the languages generated by \logicFO{} formulas,
  in the sense of~\autoref{def:language-decided-by-formula}.
  % \todo[inline]{Tutaj sie poplątałem,
  % bo zdefiniowałem moje FO jako komórki pamięci + Zero(x), One(x) - a u Immermana elementy dziedziny
  % to liczby naturalne, i jego predykaty to są normalne operacje arytmetyczne na liczbach. Nie wiem czy wyniki się przekładają...}

\begin{remark}[Bibliography]
  The theorem is originally stated in terms of \complexity{FO_{\mathrm{BIT}}[t (n)]} defined in~\cite[Definition~4.24]{Immerman1999-IMMDC}.
  We limit to \(t(n) = \bigO(1)\), i.e.\ in our case $\complexity{FO_{\mathrm{BIT}}[t (n)]} = \logicFO{}$.
  The uniformity condition is also different to the one we fixed in~\autoref{def:logspace-uniformity}:
  their circuits are so-called \complexity{FO}-uniform, which is a much stronger condition.
  For details about the different notions of uniformity, refer to~\autoref{chap:uniformity}.
  % The~\cite[Corollary~5.32]{Immerman1999-IMMDC} is implied from~\cite[Theorem~5.22]{Immerman1999-IMMDC} and earlier
  % from~\cite[Theorem~5.2]{Immerman1999-IMMDC}. It is proved that
  % \complexity{FO} is in \complexityi{AC}{0} in~\cite[Lemma~5.4]{Immerman1999-IMMDC}.
  % The proof that \complexityi{AC}{0} is in \complexity{FO} is in two steps,~\cite[Lemma~5.3]{Immerman1999-IMMDC}
  % and then~\cite[Lemma~4.25]{Immerman1999-IMMDC}.
\end{remark}
\end{theorem}


% As an example, Grädel's Theorem in descriptive complexity states that
% \(\complexity{NL}\) is the class of finite models of the second-order Krom formulas
% \cite{GRADEL199235}.
% Here, a \emph{Krom formula} is a formula in conjunctive normal form (CNF)
% where each clause contains at most
% two literals. The satisfiability problem for Krom formulas, Krom-SAT, is
% complete for \(\complexity{coNL}\) (or equivalently \(\complexity{NL}\), by the Immerman-Szelepcsényi
% Theorem).

% % second-order horn formula: https://en.wikipedia.org/wiki/Second-order_propositional_logic
% Similarly, Grädel~\cite{GRADEL199235} shows that second-order Horn formulas
% express precisely the properties decidable in polynomial time.
% A \emph{Horn clause} is a clause with at most one positive literal and any number
% of negative literals, and a Horn formula is a conjunction of such clauses.
% Similar results exist for most commonly studied complexity classes:
% \(\complexity{NP}\) (Fagin's theorem), \(\complexity{coNP}\), \(\complexity{PH}\), \(\complexity{PSPACE}\), and \(\complexity{EXPTIME}\).

% For an accessible introduction, see the classical and self-contained reference
% by Immerman~\cite{Immerman1999-IMMDC}.

% Fagin's theorem says that existential second-order logic characterizes
% \(\complexity{NP}\).
% It is worth noting that this theorem does \emph{not} assume order on the input ---
% intuitively, \(\complexity{NP}\) is strong enough to ``guess'' the order relation.




\section{The Quest for a Logic Capturing \complexity{PTIME}}\label{subsec:unordered-structures}
Many of the problems in \compP{} are graph problems, i.e.\ they ask to decide a property $\varphi(G)$
for some abstract graph $G$. It is natural to represent a graph as a logical structure:
the nodes of the graph correspond to the elements of the universe and we add to the vocabulary a special relation
$E(x, y)$ denoting there is an edge from $x$ to $y$. This is reflected by the following

\begin{definition}[Logic on graphs]\label{def:fo-unordered}
  Define the vocabulary \(\tau_{\text{graph}}\) to contain only the binary relations \(x = y, E(x, y)\). 
  The class \complexity{FO_{\text{graph}}} contains precisely the sentences of first-order logic over
  the vocabulary \(\tau_{\text{graph}}\).

  From now on, when talking about the semantics of
  sentences from $\complexity{FO}_{\text{graph}}$, we will assume the interpretation of elements of the universe
  as nodes of the graph $G$, and the interpretations of $x = y$, $E(x, y)$ to agree with
  the underlying node equality and edge relation of $G$. For example, we will assume that $E(x, y)$ holds 
  if and only if
  there is an edge between nodes of $G$ corresponding to the universe elements $x, y$.

  \begin{remark}[Bibliography]
    Typically, the vocabulary of logic on graphs also contains unary symbols $a(x), b(x), \dots$ denoting that
    the color of node $x$ is $a$ or $b$ etc.~\cites[Definition~12.2]{Immerman1999-IMMDC}[Theorem~1.36]{Immerman1999-IMMDC}.
    As we will not consider
    coloring of nodes in this thesis,
    we don't need to introduce that.
    In the literature, \(\complexity{FO}_{\text{graph}}\) is typically called $\complexity{FO (wo~\leqslant)}$,
    modulo the addition of the edge relation.
    For more details, see~\cite[Ordering~Proviso~1.14]{Immerman1999-IMMDC}
    and also discussion under~\cite[Question~12.1]{Immerman1999-IMMDC}.
  \end{remark}
\end{definition}

Defining graph problems
rarely requires us to impose any numbering on the nodes of the graph. However,
to talk about deciding a property of a graph on a Turing machine, we need to
encode the graph as a binary string. This imposes some artificial ordering on the vertices
of the graph. Perhaps a more suitable definition of logic of graphs would thus be

\begin{definition}[Logic on graphs with order]\label{def:fo-graph-ordered}
  We denote by \complexity{FO_{\text{graph}\leqslant}} the class of first-order sentences over
  $\tau_{\text{graph}}$ extended with the constants $0, 1, 
  \mathrm{max}$, the binary relations $\mathrm{BIT}, \leqslant$ and the operator $\complexity{LFP}$,
  interpreted as earlier.
\end{definition}

Now, let's consider adding the least fixed-point operator to logic on graphs.
We will denote by \logicFOLFPgraph{} the logic obtained by extending \complexity{FO_{\text{graph}}}
with the $\complexity{LFP}$ operator.
For the \complexity{FO_{\text{graph}\leqslant}} with $\complexity{LFP}$, we need to notice two things.
First, we refer to~\cite[Proposition~9.16]{Immerman1999-IMMDC} stating that
the relation $\mathrm{BIT}$ is first-order definable with ordering and $\complexity{LFP}$.
Second, we notice that the addition of the binary edge relation doesn't change the expressive power here.
Indeed, with ordering we can encode the edge relation as part of input.
Thus, we will treat this logic as equally strong to $\logicFO{}$ introduced earlier.
In particular, we will assume without
transferring the proof of~\autoref{thm:fo-lfp-eq-ptime} that $\complexity{FO_{\text{graph}\leqslant}} = \compP$.
Thus, we obtain two similarly defined theories: \logicFOLFPgraph{}, not having access to the order relation,
and \logicFOLFPgraphord{}, only operating on ordered structures.

For graph problems we typically want the Turing machine $M$ to return the same
answer regardless of the order we pass the vertices in. However, when looking at the code
of a particular Turing machine, it's usually difficult to tell if it returns the same answer
for all permutations of the input graph. The implicit assumption of being always given \emph{some} ordering
of input graph nodes, is inherent in computation on Turing machines. We may now wonder:
does this assumption limit us in the generality of programs we write? If someone forced us
to not rely on this ordering in our programs, and write programs in a \emph{permutation-invariant}
way, would anything change in our expressive power? This turns out to be a very deep and difficult question.

We will say that a graph problem $P$
is in the complexity class \complexity{inv\text{-}P} iff there is a polynomial-time Turing machine $M$
such that for every graph $G$ and every permutation $\pi$ of vertices of $G$,
\[M(\mathrm{enc}(\pi(G))) \leftrightarrow P(G).\]
That is: the problem $P(G)$ has such a decider $M$ that it returns the correct answer regardless
of how we label the input nodes. The class \complexity{inv\text{-}P} is also called \compP \emph{on unordered structures}.

As motivated earlier, \logicFOLFPgraphord{} is strong
enough to express any property from \compP. However, it turns out that when we take the order out,
\(\logicFOLFPgraph{}\) \textbf{cannot}
express every problem from~\(\complexity{inv\text{-}P}\). Equivalently: that \logicFOLFPgraph{}
cannot capture \compP~\cite{Cai1992}. This means that there are graph problems that
have a robust, order-invariant decider $M$, yet can't be expressed by a logical formula having access
to the $\complexity{LFP}$ operator, but not having access to the arithmetical predicates and ordering.
The existence of a logic that characterizes \complexity{inv\text{-}P} (or \(\complexity{P}\) on
unordered structures) remains a major open problem in computer science as of 2025.
Good overviews of this problem are~\cite{dawar2012syntactic} and~\cite[Chapter 12, The Role of Ordering]{Immerman1999-IMMDC}.

An important result that treats unordered structures is Fagin's theorem~\cite{Fagin1974} that states
that the class of languages generated by sentences of existential second-order logic is precisely \complexity{NP}.
Intuitively, ordering of the domain is not necessary to assume for Fagin's theorem because
in \complexity{NP}, we can \emph{guess} it.

% In a paper from 2019, Jean-Yves Moyen and Jakob Grue Simonsen discuss the problem of providing syntax for PTIME ,
% concluding that in the light of their generalization of Rice's theorem \cite{10.1007/978-3-030-22996-2_19}.
% (problem w reprezentacji:
% mozemy miec jezyk programowania dla P, ale odkladamy nierozstrzygalnos do problemu sprawdzenia czy mozna
% przetlumaczyc dana maszyne turinga na ten jezyk)








\section{Defining functional problems in logic}
\subsection{First-order queries (\complexity{FO}-reductions)}
Despite logic being only able to naturally define decisive problems,
some approaches have been used to reason logically about functions.
Most importantly, to study completeness of problems in low complexity classes
such as \compL, \complexity{FO}-reductions are used. They have a rather
complicated definition, which we don't display here and for the details
refer to~\cite[Definition~1.26]{IMMERMAN198686}. In the same work,
even weaker notions of reducibility are studied: first-order projections (fops) and
quantifier-free projections (qfps) are
defined in~\cite[Definition~11.7]{Immerman1999-IMMDC}.
An interesting property of the complexity classes we are studying in this work is
that their complete problems are already complete under surprisingly weak reductions.
In~\cite[Proposition~11.10]{Immerman1999-IMMDC} it is proved that
\problem{SAT} is \complexity{NP}-complete via fops and even via qfps.
% There is a problem \(\texttt{S}\) that is \complexity{NP}-complete via first-order
% reductions, but not via fops\cite[Proposition~11.14]{Immerman1999-IMMDC}.
% The following interesting property of first-order projections, which says that
% there is only one complete problem via first-order projections for each ``nice''
% complexity class.
% Let \(\texttt{C}\) be one of the complexity classes \complexityi{NC}{1}, \complexity{L}, \complexity{NL},
% \complexity{P}, \complexity{NP}, \complexity{PSPACE}.
% Let \(\texttt{A}\) and \(\texttt{B}\) be problems complete for \complexity{C} via fops. Then there is a first-order
% definable isomorvarphism between \(\texttt{A}\) and \(\texttt{B}\).

\subsection{Bit-graph definitions}\label{subsec:bit-graph-definitions}
An easy way to define a general function from Boolean functions is to
use the Boolean functions to decide if ``$i$-th bit of the output $f(x)$ is $0$ or $1$''.
For the definition to be precise, two more technicalities are needed (the output's length itself
must be polynomial and easy to compute), which we will
discuss while defining \compL-reductions in~\autoref{def:logspace-reductions}.
For now, we will just say that this style of definitions is very important for
some of the results we will study in~\autoref{chap:bounded-arithmetic}, as 
the ``semantic'' definition of definability of functions (\cites[Definition~V.4.12]{Cook_Nguyen_2010}[Definition~5.37]{CookNguyenDraft})
is precisely that.


\subsection{\complexity{FO} and \complexity{MSO} transductions}
Some works use the notion of \complexity{FO}-transductions, e.g.\ in~\cite[Section~2]{nesetril2021structuralpropertiesfirstordertransduction},
where they are also defined. They are,
however, completely different from anything we study in this work and are defined as compositions of
\emph{copying}, \emph{coloring} and \emph{simple interpretations}, which we will not discuss.
A similar, and much more important notion is of \complexity{MSO} transductions defined
e.g.\ in~\cite[Section~2]{COURCELLE199453}
% https://www.mimuw.edu.pl/~bojan/posts/who-to-cite-mso-transductions











\section{Descriptive Complexity and programming languages}\label{sec:descriptive-for-programming}
From the point of view of programming languages, the meaning of
e.g.\ the result discussed in~\autoref{subsec:unordered-structures} is as follows: given a logical formula
\(\varphi \in \logicFOLFP{}\), we \emph{know that there exists some} Turing machine
of complexity \complexity{P} that will check for us if \(w \models \varphi\) for any input \(w\).
There is a huge leap of faith, however --- just that the machine \emph{exists} and runs fast,
we can't conclude that we will ever be able to actually use it. We have to inspect
the proof of such a theorem and tell if we can compute the description itself of such a machine,
and how fast we can do it. An illustrative example is infeasibility of using the below theorem
to design a programming language for finite automata:

\begin{theorem}[{\cite[Theorem~1.36]{Immerman1999-IMMDC}}~Büchi-Elgot-Trakhtenbrot theorem]
  For the alphabet \(\Sigma := \{0, 1\}\),
  the set of boolean queries expressible in second-order monadic logic (which we skip the definition of)
  over the vocabulary \(\tau_\leqslant\) consists exactly of the regular languages. In other words, \(\complexity{MSO} = \complexity{REG}\).
  \begin{remark}[Bibliography]
    The theorem was originally proved by Büchi in~\cite{Bchi1960WeakSA}, Elgot in~\cite{8cb177c1-4df9-3823-9c81-47f75da88fff}
    and Trakhtenbrot in~\cite{Trakhtenbrot1962}.
    The statement from~\cite[Theorem~1.36]{Immerman1999-IMMDC} is slightly more accessible.
    It uses slightly different wording to ours. For the details of the definitions, refer to~\cite[Proviso~1.14]{Immerman1999-IMMDC} and~\cite[Proviso~1.15]{Immerman1999-IMMDC}.
  \end{remark}
\end{theorem}


However, not always is this problem that difficult. Results from descriptive complexity have been crucial
for the field of database theory, which we don't discuss here. The approach of using logic for programming
has also coined the paradigm of logic programming.

\subsection{Logic programming}
The most famous example of a logic programming language is Prolog. Prolog doesn't have a reputation
of a language well-suited for computational complexity analysis. However, it relates very well
with the notions of deciding computational problems we discussed in this chapter. Please see~\autoref{lst:prolog-example}
for a demonstration how we can transfer our considerations from~\autoref{exm:logic-simple-model} to practical computation.

\begin{rawlisting}
\input{listings/prolog-example.tex}
\caption{Prolog example}\label{lst:prolog-example}
\end{rawlisting}


\subsection{Datalog}
Datalog is a programming language that uses the paradigm of logic programming, similarly to Prolog.
Unlike Prolog, however, it \emph{captures} the complexity class of \compP~\cite[Theorem~4.4]{10.1145/502807.502810} (on ordered, finite structures).
That is, it is a language such that any query definable in it will be checkable in \compP{} for a given model.
We have not investigated the subtleties of Datalog's implementation and whether the complexity doesn't blow up
somewhere.



\subsection{Logic as the type system}
Could the results from descriptive complexity be used to design
a specification mechanism (such as a type system\footnote{Language doesn't have to be typed to foster
partial proofs of correctness; see e.g.~\cite{10.1145/319301.319317},~\cite{paulson2000settheoryverificationii}.})
for a programming language?
While such a language would be very interesting and possibly have highly desired
properties, our ``typechecker'' in the naive case would essentially be
a proof checker for first order logic. This would be a very difficult system to
design properly. We were not successful in curating such a small set of basic
programming instructions that,
given a pre- and a post-condition in \complexity{FO} and an operation from that set,
checking for the validity of such a triple would be an easy problem.

Nonetheless, this line of inquiry led to a broader investigation of
type systems that enforce resource bounds --- particularly those inspired
by linear logic and implicit computational complexity.
The results of this exploration are presented in~\autoref{sec:linear-types}.
