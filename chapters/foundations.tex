\chapter{Models of computation, programming paradigms and complexity measures}\label{chap:foundations}
One of the first decisions a programming language designer has to make is choosing
the programming paradigm convenient for writing the programs of interest.
In the practice of programming, imperative languages
have no competition when a user needs to reason about the computational complexity of
programs.
The structure of imperative programs closely mirrors how the computation
is executed on modern CPUs and GPUs.
In turn, it is inherently unintuitive to reason about the
complexity of programs written e.g.\ in Haskell or Prolog.

Yet if we want to understand what classes of functions can be characterized syntactically,
we have to temporarily step away from the imperative mindset. As argued in~\cite{10.1007/978-3-642-27660-6_3},
the very notion of an ``algorithm'' is still evolving, so we shouldn't limit our considerations
to a single paradigm. This chapter tests whether
alternative models of computation could be better suited
to form the basis of languages that capture popular complexity classes.

We can foreshadow that the answer is (perhaps surprisingly) positive: even though the complexity classes are defined on Turing machines,
the characterizations studied in literature are rarely imperative. 

\section{Finite automata and transducers}
Finite-state transducers compute string-to-string functions and have simple descriptions.
In~\cite{bojańczyk2018polyregularfunctions},
four characterizations are given for the class of \emph{polyregular} functions --- a~class of string-to-string
functions computed by a particular kind of transducer. The definitions described there readily
constitute the basis of a programming language. Another programming
language for transducers is studied in~\cite{DBLP:conf/fsmnlp/Schmid05}. A particular
class of string-to-string functions defined using logic, \emph{MSO transductions},
is characterized to be precisely the class of functions computable by two-way deterministic finite transducers (2DFT)
in~\cite{engelfriet1999msodefinablestringtransductions}. All of these results
provide  basis for new programming languages and are therefore more than relevant to our work.

Because transducer classes remain poorly understood, it is usually unclear whether an arbitrary problem 
belongs to the class recognized by a given flavor of transducer.
Writing programs in such a programming language would therefore often be equivalent
to doing new research on transducers, so for now we focus
on more well-established classes of functions. A~good overview of existing research on
transducers can be found in~\cite{muscholl_et_al:LIPIcs.STACS.2019.2}.

The existing research on finite-state automata has not been directly useful for our work ---
perhaps because the expressiveness of this model is inherently limited to Boolean-valued functions.

\section{Turing machines}
This model of computation underpins the imperative programming style.
There is a variety of flavors of Turing machines, and the details of a specific definition will most
of the time not affect our considerations in this work. When not explicitly stating otherwise,
when describing a computational process we will implicitly assume the realization of it on some kind
of a Turing machine.

\begin{definition}[Time complexity]\label{def:turing-dtime}
Let \(T : \mathbb{N} \to \mathbb{N}\) be some function.  A language \(L\) belongs to \(\complexity{DTIME}(T(n))\) iff there exists a deterministic Turing machine that satisfies:
\begin{enumerate}[label= (\roman*), ref= (\roman*)]
    \item\label{itm:dtime} for some constant \(c > 0\) and every input \(w \in \{0,1\}^{n}\) it terminates within \(c \cdot T(n)\) steps;
    \item for every \(w \in L\) the machine outputs \(1\);
    \item for every \(w \notin L\) the machine outputs \(0\).
\end{enumerate}
We will say that a Turing machine belongs to \(\complexity{DTIME}(T(n))\) iff it satisfies~\ref{itm:dtime}.
\end{definition}

\begin{definition}[Space complexity]\label{def:turing-dspace}
Let \(T : \mathbb{N} \to \mathbb{N}\) be some function.  A language \(L\) belongs to \(\complexity{DSPACE}(T(n))\) iff there exists a deterministic Turing machine that satisfies:
\begin{enumerate}[label= (\roman*), ref= (\roman*)]
    \item\label{itm:dspace} for some constant \(c > 0\) and every input \(w \in \{0,1\}^{n}\) it halts while using at most \(c \cdot T(n)\) work-tape cells;
    \item for every \(w \in L\) the machine outputs \(1\);
    \item for every \(w \notin L\) the machine outputs \(0\).
\end{enumerate}
We will say that a Turing machine belongs to \(\complexity{DSPACE}(T(n))\) iff it satisfies~\ref{itm:dspace}.
\end{definition}

The most popular complexity classes are directly based on the above notions.
Yet surprisingly few elegant, high-level
imperative languages are known to characterize well-studied complexity classes beyond the trivial ones.
This scarcity is precisely why much of the work surveyed in later chapters relies on non-imperative paradigms.

\subsection{Random-access Turing machines}
Random-access Turing machines are Turing machines with a special ``pointer'' tape,
of length logarithmic to the size of input, and a special state such that when the binary
number on the pointer tape is \(n\), \(n\)-th digit of the input is written to the work tape.

Reasoning about computation in complexity classes such as \(\complexity{L}\) or \(\complexity{P}\) is the same
for traditional Turing machines and random-access Turing machines. The choice of model starts to matter for
notions of complexity such as \(\complexity{DTIME}(n)\) or \(\complexity{DTIME}(n^2)\) (\emph{fine-grained} complexity classes).
Due to insufficient existing research on implicit characterizations of
fine-grained complexity classes, we will not consider them besides a brief discussion
in~\autoref{subsec:fine-grained-reductions}. Computation on transducers is a promising way to obtain robust characterizations of these
classes --- e.g.~\emph{polyregular} functions are computable in linear time by a kind of a transducer~\cite{bojańczyk2018polyregularfunctions}.
This is not a \emph{characterization}, however, because it is unlikely that every \(\complexity{DTIME}(n)\) function is definable in this language.
The notion of \(\complexity{DLOGTIME} = \complexity{DTIME}(\log n)\)-uniformity explored in~\autoref{subsec:dlogtime-uniformity} is also
only defined for random-access Turing machines.

\section{Circuits}
Circuits as a model of computation are the theoretical foundation of parallel programming.
The theoretical implications turn out to not be widely applicable in practice, however.
In this work, we will mostly use circuits to reason about very weak complexity.

As we will see in~\autoref{sec:uniformity}, we will almost always want to 
define a circuit family by a function (of a low complexity) \(n \rightarrow C_n\), computing the description
of the circuit for a given input size \(n\). If we obtain a language for
such functions, we will be able to use it to \todo{compute with complexity bound,
i.e.\ that if the resulting circuit is correct, it will be logspace-uniform ac0}compute circuit families. We will study
a logical characterization of a variant of \(\complexity{AC}^0\) in~\autoref{sec:vac0}.

A very rich \todo{there is nothing new for me
in this paper; i have also a very good literature overview, but scattered around this PDF}
overview of different characterizations of circuit complexity classes is
in~\cite{antonelli2025characterizingsmallcircuitclasses}.

\section{Discrete differential equations}
An original point of view on computation is to describe functions
as solutions to discrete differential equations. For example, in~\cite{bournez_et_al:LIPIcs.MFCS.2019.23},
\(\complexity{FP}\)~(\autoref{def:fp}) and \(\complexity{FNP}\)~(\autoref{def:fnp}) are characterized.
Characterizations of various circuit classes from
\(\complexity{FAC}^0\) to \(\complexity{FAC}^1\)~(\autoref{def:faci}) 
are described in~\cite{antonelli2025characterizingsmallcircuitclasses}.

\section{Logic programming and descriptive complexity}
Logic provides very deep complexity-theoretic connections,
primarily through descriptive complexity theory, which we explore in more detail in~\autoref{chap:descriptive-complexity}.

\section{Untyped recursion}\label{sec:untyped-lambda}
Another classical paradigm is that of general recursive functions, or equivalently the untyped
lambda calculus. Because these systems are Turing complete, the interesting question
is how to constrain recursion so that the resulting language captures a specific class.
We treat these ideas in~\autoref{chap:recursion-theory}.

\section{Typed lambda calculus}
Typed lambda calculus underpins functional programming. In this section we focus on typed variants,
unlike in~\autoref{sec:untyped-lambda}.

Lambda calculus does
not line up cleanly with traditional Turing machine complexity measures.  For instance,
for some representation of strings \(\{0, 1\}^\ast\),~\cite[Theorem~3.4]{10.5555/788018.788832} identifies
the functions \(\{0, 1\}^\ast \rightarrow \{0, 1\}\) definable in simply typed
lambda calculus (STLC), with regular languages. But with a different encoding of inputs,~\cite{HILLEBRAND1996117} relates STLC to the whole \(\complexity{ELEMENTARY}\) class. 
Moreover,~\cite{zakrzewski2007definablefunctionssimplytyped} states that
with a different ``standard'' encoding, STLC instead characterizes extended polynomials, and further shows
that if we slightly modify the encoding, the class is yet different.
For more discussion, see also~\cite{27863}.
Consequently, it is not obvious how to reason about complexity theory in the language of lambda calculus.

Nevertheless, typed lambda calculi have been utilized
very successfully to syntactically characterize complexity classes.
Recall that one of the reasons linear logic is studied is the potential
to reason about resource creation and utilization.
Concepts from linear logic have been implemented in the theory of type systems
to transfer the resource interpretation.
This will be discussed further in~\autoref{chap:linear-types}.

\section{Set theory as inspiration for model of computation}
An interesting connection appears when we think of traditional notions of ``complexity'' of sets in set theory
from the point of view of computational complexity. 
If we treat taking complements, intersections, countable unions as operations in a programming language,
perhaps we could design a programming language for constructing sets. By itself such a language would probably
not be the most interesting one. However, thinking about mathematical reasoning
in terms of a computational process is a very powerful technique. It has been deeply explored
under the name of Curry-Howard or proofs-as-programs correspondence.\footnote{A good introduction to the
immensely deep topic of proofs-as-programs is~\cite{10.5555/1197021}.} For the computational content
of set theory in particular, an interesting brief discussion is presented in~\cite{Tao2010ComputationalPerspective}.

For our purposes, an interesting connection appears when we look at descriptive set theory.
The most basic classes of sets distinguished there are open and closed sets. Slightly higher,
an \(F_\sigma\) set is a countable union of closed sets; a \(G_\delta\) set is a countable intersection
of open sets. A~few levels higher up the \emph{Boldface hierarchy}, which in a way quantify 
the complexity of sets, Borel sets are considered:

\begin{definition}[Borel sets]
Let \(X\) be a topological space. The class of Borel sets of \(X\), \(\mathcal{B}(X)\) is the smallest class of sets
containing every open set of \(X\) and closed under~\ref{itm:borel-comp} and~\ref{itm:borel-union}:
\begin{enumerate}[label= (\roman*), ref= (\roman*)]
    \item\label{itm:borel-comp} if \(A\) is Borel, then its complement \(X \setminus A\) is Borel;
    \item\label{itm:borel-union} if \(A_n\) is Borel for each \(n \in \mathbb{N}\), then the countable union \(\bigcup_{n \in \mathbb{N}} A_n\) is Borel.
\end{enumerate}
\end{definition}

As this is a standard least fixed point definition, it is strikingly similar to
definitions of classes of recursive functions that we will consider later, e.g.~\autoref{def:primitive-recursive}.
We can also take a computational point of view on theorems about the determinacy of Gale-Stewart games (which we shall not introduce here).
It is widely known that Gale-Stewart games are determined when the underlying set is open or closed.
Allowing the underlying set to be more and more complicated, we quickly 
reach the limits of provability in Zermelo-Fraenkel set theory: determinacy for Borel sets is a
difficult theorem, and determinacy for analytic and projective sets is
independent of ZF, yet provable assuming as axiom the existence of an appropriately large cardinal.

A good question to ask is if by carefully curating the axioms, we could
obtain a mathematical theory such that theorems corresponding
to computation in our desired complexity class are provable, and
the theorems that wouldn't be ``implementable'' are not.

We will circle back
to this intuition while considering the \(\texttt{PIGEON}\) computational problem in~\ref{subsubsec:tfnp} and
the unprovability of the related pigeonhole principle in the weak theories studied in~\ref{subsec:vac0-php}.
Especially the last fact about unprovability is interesting for us in this section,
as it resembles e.g.\ independence of continuum hypothesis from ZFC, but in a strictly computational setting
of not being able to perform enough computation in a low complexity class.
% This is also a hint that going this route, a collection of powerful tools used in logic to prove
% independence, will become available to us as tools for proving a problem not being solvable
% in a given complexity class.

While this line of thought is very far from ``practical programming languages'', these considerations
inspired\footnote{This line of study grew out of presenting the topic
at the JAiO master's seminar at the University of Warsaw ---
slides are available online at~\cite{Balawender2025BorelDeterminacySeminar}.}
the approach that we study in~\autoref{chap:bounded-arithmetic}.


% \section{Models of hypercomputation}
% Notes (for future work):
% \begin{itemize}
% \item Sequential Time. An algorithm determines a sequence of computational
% states for each valid input. Specifically, the time is discrete. what if time is not a successor structure? i.e. we can travel back in Time
% \item Malament-Hogarth spacetime.
% \end{itemize}
