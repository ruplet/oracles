\section{Recursion-theoretic approach}\label{sec:recursion-theory}
In this section we focus on techniques from recursion theory that were successfully
utilized in Implicit Computational Complexity.

\subsection{Origins of recursion theory}
While not the primary focus of this work, the field of recursion theory developed concepts
that later became foundational for ICC\@. An important formal system studied there is \emph{primitive recursion}.


\begin{definition}[Primitive recursive functions]\label{def:primitive-recursive}
      \(\complexity{PR}\) is the smallest class of functions containing~\ref{itm:pra-const}--\ref{itm:pra-proj} and closed under~\ref{itm:pra-comp} and~\ref{itm:pra-rec}:
\begin{enumerate}
\item\label{itm:pra-const}\textbf{(constants)} for every \(n\in\mathbb{N}\) and \(k\ge 0\), the \(k\)-ary constant function
      \(c_{n}^{(k)}(\vec x)=n\);
\item\label{itm:pra-succ} \textbf{(successor)} \(S(x)=x+1\);
\item\label{itm:pra-proj} \textbf{(projections)} for \(k\ge 1\) and \(1\le i\le k\),
      \(\pi_i^{(k)}(x_1,\dots,x_k)=x_i\);
\item\label{itm:pra-comp} \textbf{(composition)} if \(h:\mathbb{N}^m\to\mathbb{N}\) and
      \(g_1,\dots,g_m:\mathbb{N}^k\to\mathbb{N}\) are in \(\complexity{PR}\), then
      \(f(\vec x)=h\big(g_1(\vec x),\dots,g_m(\vec x)\big)\) is in \(\complexity{PR}\);
\item\label{itm:pra-rec} \textbf{(primitive recursion)} if \(g:\mathbb{N}^k\to\mathbb{N}\) and
      \(h:\mathbb{N}^{k+2}\to\mathbb{N}\) are in \(\complexity{PR}\), then the unique
      \(f:\mathbb{N}^{k+1}\to\mathbb{N}\) is in \(\complexity{PR}\) with:
      \[
      f(0,\vec x)=g(\vec x),\qquad
      f(S(y),\vec x)=h\big(y,\,f(y,\vec x),\,\vec x\big).
      \]
\end{enumerate}
\end{definition}

\begin{example}[Addition]
Define \(\mathrm{Add}:\mathbb{N}^2\to\mathbb{N}\) by primitive recursion:
\[
\mathrm{Add}(0,x)=x, \qquad
\mathrm{Add}(S(y),x)=S\big(\mathrm{Add}(y,x)\big).
\]
\end{example}

\begin{definition}[LOOP language]
Let \(\mathrm{Var}=\{x_0,x_1,x_2,\dots\}\).
LOOP programs are generated by the grammar
\[
\begin{aligned}
P ::=~& x_i := 0
\;\mid\; x_i := x_i + 1
\;\mid\; P \,;\, P
\;\mid\; \texttt{LOOP}~x_i~\texttt{DO}~P~\texttt{END},
\end{aligned}
\]
where \(x_i\in\mathrm{Var}\).

\noindent
We assume standard semantics, with a remark that \(\texttt{LOOP}~x_i~\texttt{DO}~P~\texttt{END}\) repeats \(P\) exactly as many times as the value stored in \(x_i\) \emph{at loop entry} (changes to \(x_i\) inside \(P\) do not change the iteration count).
\end{definition}

\begin{theorem}[{\cite{10.1145/800196.806014}}~\complexity{LOOP} captures precisely \complexity{PR}]\label{thm:loop-captures-pr}
      % TODO: make statement precise, perhaps based on https://www.cs.cornell.edu/courses/cs6110/2012sp/MeyerAndRitchie-67.pdf
      The functions computable by \complexity{LOOP} programs are precisely the primitive recursive functions
\end{theorem}

This simple connection actually satisfies our criteria of a ``programming language capturing a complexity class'', as
the \(\complexity{LOOP}\) language captures exactly \(\complexity{PR}\)\footnote{As recognized at \url{https://complexityzoo.net/Complexity_Zoo:P}.}, the class of primitive recursive functions.
Moreover, we can even stratify the primitive recursive functions into a hierarchy like in~\cite{Grzegorczyk1953}.

\begin{remark}[Bibliography]
      Historically, the origins of primitive recursion can be traced back to~\cite{Grassmann1861} and~\cite{Dedekind1888},
      but the class was probably first considered as the primary object of study in~\cite{Skolem1923-vanHeijenoort}.
      For the details of the historical origins, consult~\cite{Adams2011}.
\end{remark}


\subsection{Explicit characterizations}\label{subsec:explicit}
Some characterizations discussed in the literature contain so-called \emph{explicit}
conditions --- e.g.\ they explicitly require a function to not grow faster than polynomially.
Such conditions cannot be checked syntactically and must be enforced
by an additional proof outside of the algebra. We call these characterizations
explicit, as opposed to implicit. Despite not being convenient to use for our purpose, these concepts
have been very important for the field and we will discuss one example in this subsection.
For more examples of explicit characterizations, please see~\cite{4568079} for an algebra for polynomial-time
functions;~\cite{COMPTON1990241} for uniform \complexityi{NC}{1};~\cite{ALLEN19911} for uniform \complexity{NC}.

For a good overview of implicit versus explicit characterizations, refer to~\cite{aubert:hal-01111737}.

A famous example of an explicit characterization is Cobham's algebra
for polynomial-time functions, using the recursion scheme defined below.
We will use the notation \(S_0(y)=2y\) and \(S_1(y)=2y+1\) for appending a binary digit to \(y\).
We will denote by $\len{x}$ the length of binary representation of $x$; in particular, $\len{0} = 1, \len{1}=1, \len{2}=2$.

\begin{definition}[{\cite[Definition~VIII.2.14~(Page~174)]{Odifreddi1999CRT2}}]\label{def:bounded-binary-primitive-recursion}
A function \(f\) is defined from functions \(g\), \(h_0\), \(h_1\), and \(s\) by \emph{bounded primitive recursion on binary notation}
if, for every \(\vec x\) and \(y\in\mathbb{N}\),
\begin{align}
  f(\vec x, 0)      &= g(\vec x);\\
  f(\vec x, S_0(y)) &=
    \begin{cases}
      0 & \text{if } y = 0,\\
      h_0\big(\vec x, y, f(\vec x, y)\big) & \text{otherwise;}
    \end{cases}\\
  f(\vec x, S_1(y)) &= h_1\big(\vec x, y, f(\vec x, y)\big);\\
  f(\vec x, y)      &\le s(\vec x, y)\label{eq:explicit-cobham}.
\end{align}
\end{definition}

\begin{remark}\label{remark:cobham-recursion}
  The recursion parameter $y$ is written in binary, so the definition of $f(\vec x,y)$
  unfolds only $\mathcal{O}(\len{y})$ many steps.
  Moreover, the
  side condition $f(\vec x,y)\le s(\vec x,y)$ implies that every value of $f(\vec x,y)$
  is at most $s(\vec x,y)$. Hence, if the defining functions
  $g,h_0,h_1$ and the bounding function $s$ are polynomially bounded (in the lengths
  of their arguments in binary), then $f$ will also be polynomially bounded.
  
  In this definition, the
  bound~\eqref{eq:explicit-cobham} is \emph{explicit}: when one writes a function in this
  style, there is no obvious way to check mechanically that $f(\vec x,y)\le s(\vec x,y)$
  holds, other than by supplying a separate mathematical proof.
\end{remark}


\begin{definition}[Cobham's algebra for \complexity{FP}]\label{def:cobham}
The class \complexity{Cob} is the smallest class of functions containing~\ref{itm:cobham-zero}--\ref{itm:cobham-smash} and closed
under composition and bounded primitive recursion on binary notation:
\begin{enumerate}
\item\label{itm:cobham-zero} for every $k \ge 0$, the $k$-ary constant function $0(\vec{x}) = 0$;
\item the binary successor functions \(S_0(x)=2x\) and \(S_1(x)=2x+1\);
\item for \(k\ge 1\) and \(1\le i\le k\), projections
                  \(\pi_i^{(k)}(x_1,\dots,x_k)=x_i\);

\item\label{itm:cobham-smash} the weak exponential \((x,y)\mapsto x^{\len{y}}\) denoted $x\mathbin{\#}y$; sometimes called \emph{the smash function}.
\end{enumerate}
\end{definition}

\begin{remark}
This algebra was originally defined for decimal digits.
Note that all of our initial functions are polynomially bounded in the lengths
of their arguments. For the smash function we have, for $x,y \ge 1$,
\[
  x < 2^{\len{x}},
  \qquad
  x^{\len{y}} < 2^{\len{x}\cdot\len{y}},
  \qquad
  \len{x^{\len{y}}} \le \len{x}\cdot\len{y} + 1,
\]
hence
\[
  \len{x \mathbin{\#} y}
  = \len{x^{\len{y}}}
  \le \len{x}\cdot\len{y} + 1.
\]
Composition preserves polynomial boundedness, and bounded recursion on binary
notation also preserves it (recall~\autoref{remark:cobham-recursion}),
so every function in $\complexity{Cob}$ is polynomially bounded.
\end{remark}

\begin{theorem}[{\cite[Proposition~VIII.2.15~(Page~175)]{Odifreddi1999CRT2}}]\label{thm:cobham-is-fp}
      The class \complexity{Cob} contains precisely the functions from \complexity{FP}.
\end{theorem}

\begin{remark}
      Cobham's characterization underlies the arithmetical theory \complexity{PV},
      which is also designed to capture
      polynomial time reasoning in the style discussed in~\autoref{chap:bounded-arithmetic}.
      We will not introduce \complexity{PV} in this work, but we will mention it once again in~\autoref{sec:bounded-arith-imppv}
      to discuss another concept for a programming language. Please see~\cite[End~of~section~6]{10.1007/BF01201998} for
      a brief discussion.
\end{remark}

\begin{remark}[Bibliography]
This algebra from~\autoref{def:cobham} was originally published in~\cite{Cobham1964-COBTIC} and never proved by the author to
actually capture \complexity{FP}.
In~\cite[Remark~15.3.22]{Tourlakis2022Computability} there is a discussion of several proofs of this
theorem from the literature, each based on different, and in general non-equivalent, definitions.
The proof we referenced in the header of~\autoref{thm:cobham-is-fp} is due to Odifreddi.
\end{remark}



\subsection{Characterization of \complexity{FP} with safe-recursion}

Bellantoni and Cook introduced a function algebra \(\complexity{BC}\) whose key
innovation is the separation of arguments into \emph{normal} inputs (controlling
recursion depth) and \emph{safe} inputs (being passed around without
influencing that depth).
We write \(f(\vec{x};\vec{a})\), with normal inputs \(\vec{x}\) to the left of
the semicolon and safe inputs \(\vec{a}\) to the right.

\begin{definition}[Bellantoni and Cook's algebra for \complexity{FP}]\label{def:bellantoni-cook}
We will use the notation $a0$ for the binary representation of $2*a$. Similarily, we will use $a1$ for $2*a + 1$,
and the more general $ai$ when $i$ is known from the context to be equal to either $0$ or $1$.

      The class \(\complexity{BC}\) is the smallest class of functions on non-negative
integers that contains~\ref{itm:bc-constant}--\ref{itm:bc-cond} and is closed under~\ref{itm:bc-rec} and~\ref{itm:bc-comp}:
\begin{enumerate}
  \item\label{itm:bc-constant}\textbf{(constant)} \(0(;)=0\);
  \item\textbf{(projection)} for \(m,n\ge 0\) and \(1\le j\le m+n\),
        \[
          \pi_j(x_1,\dots,x_m;\,a_1,\dots,a_n)=
          \begin{cases}
            x_j     & \text{if } j\le m,\\
            a_{j-m} & \text{otherwise.}
          \end{cases};
        \]
  \item \textbf{(successors)} \(s_i(;a)=2a+i\) for \(i\in\{0,1\}\);
  \item \textbf{(predecessor)} \(p(;0)=0\) and \(p(;ai)=a\);
  \item\label{itm:bc-cond}\textbf{(conditional)}\[
        C(;a,b,c)=
        \begin{cases}
          b & \text{if } a\bmod 2 = 0,\\
          c & \text{otherwise};
        \end{cases}
        \]
  \item\label{itm:bc-rec}\textbf{(predicative recursion on notation (\emph{safe} recursion))}\
        if \(g,h_0,h_1\in\complexity{BC}\), then also \(f \in \complexity{BC}\) with
        \begin{align*}
        f(0,\vec{x};\vec{a}) &= g(\vec{x};\vec{a}),\\
        f(yi,\vec{x};\vec{a}) &= h_i\bigl(y,\vec{x};\vec{a},f(y,\vec{x};\vec{a})\bigr)\qquad \text{for } i \in \{0, 1\}
        \end{align*}
  \item\label{itm:bc-comp}\textbf{(safe composition)}\
        if \(h,\vec{r},\vec{t}\in\complexity{BC}\) with each component of
        \(\vec{r}\) taking only normal arguments and each component of
        \(\vec{t}\) taking both normal and safe arguments, then also \(f \in \complexity{BC}\) with
        \[
          f(\vec{x};\vec{a}) =
          h\bigl(\vec{r}(\vec{x};\,);\ \vec{t}(\vec{x};\vec{a})\bigr).
        \]
\end{enumerate}
\end{definition}

\begin{theorem}[{\cite[Theorem~3.3,4.2]{10.1007/BF01201998}}]\label{thm:icc-belantoni-cook-fp}
      Let $f(\vec{x})$ be a function in \complexity{FP}. Then $f(\vec{x};)\in\complexity{BC}$.
      Let $f(\vec{x};\vec{y})$ be a function in \complexity{BC}. Then $f(\vec{x}, \vec{y}) \in \complexity{FP}$.
\end{theorem}


\begin{remark}[Intuition]
  The key idea of safe recursion is that safe arguments can never flow back into normal positions.
  In safe composition, the normal arguments of $h$ are obtained only from
  functions that themselves take only normal inputs. In the recursion scheme, the
  recursive value $f(y,\vec{x};\vec{a})$ is passed to $h_i$ as a safe argument.
  As a result, the
  depth of recursion can depend only on the normal inputs, and not on values computed
  during the recursion.
\end{remark}

\begin{remark}\label{remark:icc-bc-algebra-bitstrings}
The theorem is formulated in terms of computation on non-negative integers, but the proof transfers to
the case of general binary strings (e.g.\ being able to start with a zero)~\cite{10.1007/BF01201998}.
\end{remark}

\begin{remark}[Formalization of polynomial time functions]\label{remark:heraud-nowak}
Interestingly, in~\cite{10.1007/978-3-642-22863-6_11} the authors claim formalizing a~proof that
the Bellantoni-Cook algebra formulated on binary strings (recall~\autoref{remark:icc-bc-algebra-bitstrings})
captures precisely \complexity{FP}.


This sparked our interest, as it could suggest that the authors have formalized the notion of an \complexity{FP}
function in a proof assistant. In particular, one could hope for a compiler translating \complexity{BC} programs
into Turing machines running in \complexity{FP}. This is not the case, however, as
they have only formalized the proof that the class \complexity{BC} is the same as the class
\complexity{Cob}.\footnote{Link to the source code: \url{https://github.com/davidnowak/bellantonicook}.}
\end{remark}


\begin{remark}
In~\cite{10.1007/BF01201998} it is also shown how to readily use their safe recursive algebra
to characterize functions from \complexity{FL} with ``small output'', but this characterization
relied on using unary representation of natural numbers on input, which is more of a 
hack than a true characterization of this class.
\end{remark}


\subsection{Characterization of \complexity{FL} with affine safe recursion}
MÃ¸ller-Neergaard refined the safe recursion discipline to capture \complexity{FL}
by strengthening the treatment of safe data: each safe value may be \emph{used} at most once.
An analogy from quantum computing is that after we ``measure'' a value by, for example, testing
one of its bits in a conditional, it disappears (goes out of scope). As in the
Bellantoni-Cook setting, arguments are split into normal and safe ones, and recursion is
permitted only on the normal arguments. In addition, the composition and recursion schemes are
designed so that safe arguments are never duplicated, and recursion has a course-of-value
flavour: successive recursive calls are allowed to ``jump back'' along the recursion chain and
do not need to visit every intermediate value. These restrictions together yield a function
algebra that characterizes \complexity{FL}. As the precise definition is quite technical and
will not be used later, we refer the interested reader to~\autoref{def:bc-eps}
and~\autoref{thm:neergaard-theorem} in~\autoref{chap:appendix-neergaard}.

This algebra was very important for us, as its description in~\cite{10.1007/978-3-540-30477-7_21}
explicitly names it as a programming language. It seemed very promising indeed to be implemented
on a computer as a simple, useful programming language.

However, in practice we found it very difficult to program in this algebra.
Neergaard's system can define every function in \complexity{FL}, but not every
\emph{algorithmic technique} commonly used to implement \complexity{FL} Turing machines.
In other words, it matches \complexity{FL} extensionally, but its intensional
expressive power (cf.~\autoref{subsec:intensional}) is quite limited. For instance,
even a function that always returns $0$ requires a non-trivial use of the recursion
scheme, simply to introduce normal arguments.

It seemed unlikely that we could add support for structures such as pairs and lists to this programming language.
In an unpublished technical report by the author, accessible at~\cite{Neergaard2004BCeps},
it was discussed that the type of \emph{pairs} of numbers seems not to be implementable in this
language. The property of the whole data structure \emph{disappearing}
after we check one bit of it seems to render implementing typical data structures very difficult.


\paragraph{Neergaard's original code from 2004}
The author's original code of the interpreter referenced in the paper is not publicly accessible.
We have managed to obtain the original Moscow ML code from 2004 from the author on a permissive license,
port it to a modern version of SML/NJ and release it with the author's
permission.\footnote{The code is available at:~\url{https://github.com/ruplet/neergaard-logspace-characterization}.}

\begin{remark}[Bibliography]
      The technical report~\cite{Neergaard2004BCeps} has never been published and seems to be
      a preliminary version of the author's publication from the same year. Despite that,
      it contains crucial insight on the limits of this characterization.
      There are papers closely related to Neergaard's publication and necessary
      to also be studied while exploring Neergaard's work. Please also
      see~\cite{NeergaardMairson2003HowLight}. There is also~\cite{MurawskiOng2000SafeRecursion},
      but it seems inaccessible online. Probably the contents of that work is similar to~\cite{MURAWSKI2004197}
\end{remark}

\begin{remark}[Origins of this thesis]
For insight into the timeline of this thesis, it's worth to note that we have first discovered
this paper ourselves around February 2023.
\end{remark}